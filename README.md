
# Multilayer Input Sanitizer for Large Language Models (LLM) in n8n

This repository contains an **n8n workflow** (`n8n/workflows/sanitizer.workflow.json`) and supporting documentation (`docs/index.md`).
The workflow implements a **multi-layered input sanitization pipeline** to enhance the safety and robustness of interactions with Large Language Models (LLMs).

## ğŸš€ Quick Start

1. **Import the workflow** into n8n:
   Go to **Workflows â†’ Import from file** and select `n8n/workflows/sanitizer.workflow.json`.
2. **Configure credentials & environment variables**
   Adjust any required settings (API keys, environment variables, etc.) before running.
3. **Run & monitor**
   Execute the workflow and observe logs to verify correct sanitization and behavior.

## âœ¨ Features

* **Normalization** â€“ cleans input (trimming, Unicode normalization, removal of control characters).
* **Rule-based filters** â€“ detects and blocks common attack patterns (e.g., prompt injection attempts).
* **Heuristics** â€“ lightweight checks for suspicious or manipulative input.
* **Optional classifiers** â€“ integration with toxicity/safety models.
* **PII handling** â€“ redact or mask sensitive personal data before forwarding to the LLM.

---


## ğŸ“‚ Repository Structure

```text
.
â”œâ”€â”€ n8n/workflows/sanitizer.workflow.json  # Main workflow
â”œâ”€â”€ docs/index.md                          # Project documentation
â”œâ”€â”€ examples/                              # Usage examples (optional)
â””â”€â”€ .github/workflows/ci.yml               # Basic CI: JSON + Markdown validation
```
---

## ğŸ“– Documentation

Detailed usage and design notes can be found in [docs/index.md](docs/index.md).

## ğŸ¤ Contributing

Contributions, ideas, and feedback are welcome!
Please open an issue or a pull request if youâ€™d like to help improve this project.

## ğŸ“œ License
Released under the [MIT License](LICENSE).
